{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJt1q4eOZPhF"
      },
      "source": [
        "# 8.3 Exercise\n",
        "\n",
        "In the first part of this practical, we developed an RNN-based machine translation algorithm, while in the second part, we tackled the sentiment analysis task using Transformers.\n",
        "\n",
        "Let's now combine these two practicals and develop a Transformer-based machine translation method.\n",
        "\n",
        "Remember to use the Pytorch implementation of [Transformers](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html).\n",
        "You do not need to change anything else in this notebook, except implementing the neural network.\n",
        "\n",
        "However, please note that there are several small changes in this notebook compared to the first one. Although there are some comments identifying these in the code, it might be a good idea to study and understand the code again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvefjFMbfdtr"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ou_7pECcTHf"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset. Note that now we have `pad` and `unk` tokens."
      ],
      "metadata": {
        "id": "ygRz_grLznxs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgl44v3KZ9nO"
      },
      "outputs": [],
      "source": [
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # SOS = start of sentence\n",
        "EOS_token = 2  # EOS = end of sentence\n",
        "UNK_token = 3  # UNK = unknown token\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}  # we will use the index to create the one-hot vector\n",
        "        self.n_words = 4  # Count special tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTczbAaDZ9nR"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is21Aw4HZ9nT"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('lab8-%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1yhaqjdZ9nW"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL8hdjrPZ9nX"
      },
      "outputs": [],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dETJmHQ1d71v"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence, target=False):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    if target is False:\n",
        "        indexes.append(EOS_token)\n",
        "    else:\n",
        "        indexes.insert(0, SOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1], target=True)\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_text, output_text = zip(*batch)\n",
        "    input_text = pad_sequence(input_text, batch_first=True, padding_value=PAD_token) # padding sequence\n",
        "    input_padding_mask = (input_text == PAD_token)  # create mask to ignore the pad tokens - 1/True indicates pad, 0/False indicates not pad\n",
        "    output_text = pad_sequence(output_text, batch_first=True, padding_value=PAD_token)\n",
        "    output_padding_mask = (output_text == PAD_token)\n",
        "    return input_text, input_padding_mask, output_text, output_padding_mask\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH+1), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH+1), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)  # adding EOS token for input sentence\n",
        "        tgt_ids.insert(0, SOS_token)  # adding SOS token for input sentence\n",
        "        tgt_ids.append(EOS_token)  # adding EOS token for target sentence\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, collate_fn=collate_fn)\n",
        "    return input_lang, output_lang, train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0DCFs6FL_mv"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGPqZDTofE5t"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement your model here!"
      ],
      "metadata": {
        "id": "aF1vSnYY0HvZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-omA7fsfNRe"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size=512,\n",
        "        src_vocab_size=input_lang.n_words,\n",
        "        trg_vocab_size=output_lang.n_words,\n",
        "        src_pad_idx=PAD_token,\n",
        "        num_heads=8,\n",
        "        num_encoder_layers=4,\n",
        "        num_decoder_layers=4,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1,\n",
        "        max_len=MAX_LENGTH+1\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)  # word emb\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)  # pos emb\n",
        "\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)  # word emb\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)  # pos emb\n",
        "\n",
        "        # define your transformer here\n",
        "\n",
        "\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def forward(self, src, src_pad_mask, trg, trg_pad_mask, train=True):\n",
        "        N, src_seq_length = src.shape\n",
        "        N, trg_seq_length = trg.shape\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(src_seq_length, N).transpose(0, 1)\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length)\n",
        "            .unsqueeze(1)\n",
        "            .expand(trg_seq_length, N).transpose(0, 1)\n",
        "            .to(device)\n",
        "        )\n",
        "\n",
        "        embed_src = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        )\n",
        "        embed_trg = self.dropout(\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "        )\n",
        "\n",
        "        if train is True:\n",
        "            # this generates the attention mask so the decoder does not \"look into the future\"\n",
        "            trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(device)\n",
        "        else:\n",
        "            # no need to generate the attention mask during inference\n",
        "            trg_mask = None\n",
        "\n",
        "        # call your transformer here\n",
        "        out = None\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdnUl4X1LYCK"
      },
      "outputs": [],
      "source": [
        "model = Transformer().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFiw4zjOfBa0"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hgt0uPaZ9nf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that here, we shift the target sequence to the right to enable teacher forcing.\n",
        "\n",
        "Suppose the translated target sentence during training is:\n",
        "\n",
        "`['SOS', 'I', 'love', 'artificial', 'intelligence', 'EOS']`\n",
        "\n",
        "In this case, the input to the decoder should be:\n",
        "\n",
        "`['SOS', 'I', 'love', 'artificial', 'intelligence']` (no `EOS`)\n",
        "\n",
        "While the expected output is:\n",
        "\n",
        "`['I', 'love', 'artificial', 'intelligence', 'EOS']` (no `SOS`)\n",
        "\n",
        "Therefore, when the decoder sees the first word `SOS`, it should predict `I`.\n",
        "when it sees the word `I`, it should predict `love`, and so on."
      ],
      "metadata": {
        "id": "mxZprWlW1HPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHYeEdNSZ9nf"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, out_voc_size, model, optimizer, criterion):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, input_pad_mask, target_tensor, target_pad_mask = data\n",
        "        mask = (target_tensor == EOS_token)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_tensor, input_pad_mask,\n",
        "                        target_tensor[~mask].view(target_tensor.shape[0], -1),  # Remove last token for teacher forcing\n",
        "                        target_pad_mask[~mask].view(target_pad_mask.shape[0], -1))  # Remove last token for teacher forcing\n",
        "\n",
        "        loss = criterion(outputs.reshape(-1, out_voc_size),\n",
        "                         target_tensor[:, 1:].reshape(-1))  # shift target\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlZxuBWlZ9ng"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, out_voc_size, model, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, out_voc_size, model, optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otAmBYMoZ9nj"
      },
      "outputs": [],
      "source": [
        "train(train_dataloader, output_lang.n_words, model, 100, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnlLciFLMOBQ"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wus0mjuZ9nh"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, sentence, input_lang, output_lang):\n",
        "    model.eval()\n",
        "    input_tensor = tensorFromSentence(input_lang, sentence).to(device)\n",
        "    EOS_cuda = torch.tensor([[EOS_token]]).to(device)\n",
        "    input_tensor = torch.cat([input_tensor, EOS_cuda], dim=1).to(device)  # including EOS in the input sentence\n",
        "\n",
        "    outputs = [SOS_token]  # the output/translated sentence only starts with SOS\n",
        "    for i in range(MAX_LENGTH):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor, None, trg_tensor, None, train=False)\n",
        "\n",
        "        best_guess = output.argmax(2)[0, -1].item()\n",
        "        if best_guess == EOS_token:\n",
        "            break\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "    translated_sentence = [output_lang.index2word[idx] for idx in outputs]\n",
        "    # remove start token\n",
        "    return translated_sentence[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYIhD_nIZ9ni"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(model, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(model, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVHfqwVOZ9nl"
      },
      "outputs": [],
      "source": [
        "evaluateRandomly(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}