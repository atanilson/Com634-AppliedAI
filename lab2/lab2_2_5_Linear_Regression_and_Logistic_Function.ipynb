{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tycVPwWs0mv"
      },
      "source": [
        "# 5. Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUKvaUcs0mv"
      },
      "source": [
        "Linear regression models are a good starting point for regression tasks.\n",
        "Such models are popular because they can be fit very quickly, and are very interpretable.\n",
        "You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior.\n",
        "\n",
        "In this section we will start with a quick intuitive walk-through of the mathematics behind this well-known problem, before seeing how before moving on to see how linear models can be generalized to account for more complicated patterns in data.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX-5n7pJs0mw"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N741x6Y8s0mx"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "We will start with the most familiar linear regression, a straight-line fit to data.\n",
        "A straight-line fit is a model of the form\n",
        "$$\n",
        "y = ax + b\n",
        "$$\n",
        "where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n",
        "\n",
        "Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mAij_mNs0my"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(1)\n",
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x - 5 + rng.randn(50)  # slope=2 and intercept=5 in this case\n",
        "plt.scatter(x, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSWcldhcs0mz"
      },
      "source": [
        "We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cVavZfs0m0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression(fit_intercept=True)\n",
        "\n",
        "model.fit(x[:, np.newaxis], y)\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)\n",
        "yfit = model.predict(xfit[:, np.newaxis])\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, yfit);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8o81Ym9s0m1"
      },
      "source": [
        "The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore.\n",
        "Here the relevant parameters are ``coef_`` and ``intercept_``:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1Dpp5Zps0m1"
      },
      "outputs": [],
      "source": [
        "print(\"Model slope:    \", model.coef_[0])\n",
        "print(\"Model intercept:\", model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsJofq7Cs0m1"
      },
      "source": [
        "We see that the results are very close to the inputs, as we might hope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_NA2t1ys0m2"
      },
      "source": [
        "The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form\n",
        "$$\n",
        "y = a_0 + a_1 x_1 + a_2 x_2 + \\cdots\n",
        "$$\n",
        "where there are multiple $x$ values.\n",
        "Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.\n",
        "\n",
        "The multidimensional nature of such regressions makes them more difficult to visualize, but we can see one of these fits in action by building some example data, using NumPy's matrix multiplication operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QptGupss0m2"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(1)\n",
        "X = 10 * rng.rand(100, 3)\n",
        "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
        "\n",
        "model.fit(X, y)\n",
        "print(model.intercept_)\n",
        "print(model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUVrO1b9s0m3"
      },
      "source": [
        "Here the $y$ data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.\n",
        "\n",
        "In this way, we can use the single ``LinearRegression`` estimator to fit lines, planes, or hyperplanes to our data.\n",
        "It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GgG5Eyds0m3"
      },
      "source": [
        "## Basis Function Regression\n",
        "\n",
        "One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to *basis functions*.\n",
        "The idea is to take our multidimensional linear model:\n",
        "$$\n",
        "y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \\cdots\n",
        "$$\n",
        "and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.\n",
        "That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.\n",
        "\n",
        "For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:\n",
        "$$\n",
        "y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\cdots\n",
        "$$\n",
        "Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.\n",
        "What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rl4yLe2s0m4"
      },
      "source": [
        "### Polynomial basis functions\n",
        "\n",
        "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saW9C3Vms0m4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "x = np.array([2, 3, 4])\n",
        "poly = PolynomialFeatures(3, include_bias=False)\n",
        "poly.fit_transform(x[:, None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qem_8yxfs0m4"
      },
      "source": [
        "We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value.\n",
        "This new, higher-dimensional data representation can then be plugged into a linear regression.\n",
        "\n",
        "The cleanest way to accomplish this is to use a pipeline.\n",
        "Let's make a 7th-degree polynomial model in this way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrgvR2Ghs0m4"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "poly_model = make_pipeline(PolynomialFeatures(7),\n",
        "                           LinearRegression())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSNFIy-Rs0m5"
      },
      "source": [
        "With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$.\n",
        "For example, here is a sine wave with noise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MN3W81ws0m5"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(1)\n",
        "x = 10 * rng.rand(50)\n",
        "y = np.sin(x) + 0.1 * rng.randn(50)\n",
        "\n",
        "poly_model.fit(x[:, np.newaxis], y)\n",
        "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, yfit);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZO6vhpXs0m5"
      },
      "source": [
        "Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjNTviif4XHm"
      },
      "source": [
        "\n",
        "# Logistic function\n",
        "\n",
        "Unlike other regressions, Logistic Regression is used for classification and not for regression.\n",
        "Let's classify some data using Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22yu-YKG4fw9"
      },
      "outputs": [],
      "source": [
        "# Generate a toy dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.normal(size=100)  # 100 samples\n",
        "y = (X > 0).astype(float)  # binary dataset, y=1 if X>0, otherwise y=0\n",
        "X[X > 0] *= 4\n",
        "X += 0.3 * np.random.normal(size=100)\n",
        "X = X[:, np.newaxis]\n",
        "\n",
        "plt.scatter(X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train our Logistic Regression model. Read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ],
      "metadata": {
        "id": "8Ze-FHkC2qhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import expit\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Fit the classifier\n",
        "clf = LogisticRegression(C=1e5)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# and plot the result\n",
        "plt.clf()\n",
        "plt.scatter(X.ravel(), y, label=\"example data\")\n",
        "\n",
        "X_test = np.linspace(-5, 10, 300)\n",
        "loss = expit(X_test * clf.coef_ + clf.intercept_).ravel()  # getting values for the line based on the parameters learned by the model\n",
        "plt.plot(X_test, loss, label=\"Logistic Regression Model\", color=\"red\", linewidth=3)\n",
        "\n",
        "plt.legend(\n",
        "    loc=\"lower right\",\n",
        "    fontsize=\"small\",\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5dXwbWoC2l2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train a linear regression using the same data and then plot the linear regression versus the logistic regression for comparison.\n",
        "Remember that these regressions have different purposes and this comparison is just to reinforce that."
      ],
      "metadata": {
        "id": "U5lqf7BY2AB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this first part was copied from the previous cell\n",
        "plt.scatter(X.ravel(), y, label=\"example data\")\n",
        "X_test = np.linspace(-5, 10, 300)\n",
        "loss = expit(X_test * clf.coef_ + clf.intercept_).ravel()  # getting values for the line based on the parameters learned by the model\n",
        "plt.plot(X_test, loss, label=\"Logistic Regression Model\", color=\"red\", linewidth=3)\n",
        "\n",
        "# linear regression starts here!\n",
        "ols = LinearRegression()\n",
        "ols.fit(X, y)  # train linear regression\n",
        "plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, label=\"Linear Regression Model\", linewidth=2, color=\"black\")\n",
        "plt.axhline(0.5, color=\".5\")\n",
        "\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.xticks(range(-5, 10))\n",
        "plt.yticks([0, 0.5, 1])\n",
        "plt.ylim(-0.25, 1.25)\n",
        "plt.xlim(-4, 10)\n",
        "plt.legend(\n",
        "    loc=\"lower right\",\n",
        "    fontsize=\"small\",\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sfjooLUD2UDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}