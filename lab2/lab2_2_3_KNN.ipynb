{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtPCm4JgBIFX"
      },
      "source": [
        "# 3. KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLAw6JsbBIFb"
      },
      "source": [
        "For the KNN algorithm, we will take a deeper look at how to generate synthetic data, something we are already using.\n",
        "After, we will use the generated dataset to implement the algorithm and evaluate the results. Step by step, we will:\n",
        "1. Create synthetic datasets consisting of two classes of objects;\n",
        "2. Develop the k-NN algorithm;\n",
        "3. Evaluate the algorithm's ability to separate the two classes of objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMZu2gpRBIFb"
      },
      "source": [
        "## Step 1. Create synthetic dataset with two different classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iZWdYRIBIFc"
      },
      "source": [
        "Firstly, we need to generate training and validation/testing datasets for binary classification.\n",
        "For visualisation purposes, we will assume that our objects have *2 numeric features*.\n",
        "We would like to generate 2 \"cloulds\" of points on the plane corresponding to the positive and negative objects respectively. To do this, one can generate random points from a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) (function ``np.random.multivariate_normal``). For example, ``np.random.multivariate_normal([a,b], [[1,0],[0,1]], N)`` will generate a set on N points scattered around the *mean* point $(a,b)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv_xW2_HBIFc"
      },
      "source": [
        "1. Create two sets of N=20 points. The first set scattered around the point ``(1,1)`` and the second scattered around the point ``(3,3)``. These sets of points will correspond to the positive and the negative class respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRWFISJABIFc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "N = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I3rDq9YBIFc"
      },
      "outputs": [],
      "source": [
        "positive = np.random.multivariate_normal([1,1], [[1,0],[0,1]], N)\n",
        "negative = np.random.multivariate_normal([3,3], [[1,0],[0,1]], N)\n",
        "\n",
        "print(positive.shape, negative.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIWorof8BIFd"
      },
      "source": [
        "2. Plot the generated sets of points. Use different colours or markers for different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScYjkxWDBIFd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(positive[:,0], positive[:,1], c='r')  # red\n",
        "plt.scatter(negative[:,0], negative[:,1], c='g')  # green\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htiR_eZeBIFd"
      },
      "source": [
        "3. Split each of the sets into equal train and validation portions. As a result you should have four sets:\n",
        "- positive object/sample in the train dataset;\n",
        "- positive object/sample in the validation dataset;\n",
        "- negataive object/sample in the train dataset;\n",
        "- negataive object/sample in the validation dataset;\n",
        "\n",
        "To confirm that the sets have equal numbers of objects, print the number of elements in each set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M8bjuyfnisE"
      },
      "outputs": [],
      "source": [
        "halfN = int(N/2)\n",
        "train_positive = positive[:halfN,:]\n",
        "validation_positive = positive[halfN:,:]\n",
        "\n",
        "train_negative = negative[:halfN,:]\n",
        "validation_negative = negative[:halfN,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iur5EAO5nisJ"
      },
      "outputs": [],
      "source": [
        "print(len(train_positive), len(validation_positive), len(train_negative), len(validation_negative))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uePmNiKgBIFe"
      },
      "source": [
        "4. Add an extra freature (representing the class label: +1 for the positive class, -1 for the negative class) to the train and validation instances. As a result you will have two datasets, each consisting of tuples (label, instance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCYl3vQWnisS"
      },
      "outputs": [],
      "source": [
        "train_data = [(1, x) for x in train_positive]\n",
        "train_data.extend([(-1, x) for x in train_negative])\n",
        "\n",
        "validation_data = [(1, x) for x in validation_positive]\n",
        "validation_data.extend([(-1, x) for x in validation_negative])\n",
        "\n",
        "# To see that we have properly made tuples of (label, instance) lets print train and validation data.\n",
        "print(\"Train data:\\n\", train_data)\n",
        "print(\"\\nValidation data:\\n\", validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO0oX8ruBIFe"
      },
      "source": [
        "## Step 2. Develop the k-NN algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EQnKaSyBIFe"
      },
      "source": [
        "We use the training dataset from the previous step to implement k-NN prediction algorithm.\n",
        "We choose cosine similarity as a \"measure of distance\".\n",
        "The larger the similarity between two objects, the closer the objects are to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wECY1E1jnisj"
      },
      "source": [
        "1. Create the cosine similarity function that will be used in the k-NN prediction function to find the neighbours. The function should take two vectors as input and output the cosine similarity between the vectors: $\\operatorname{cosSimilarity}(p,q)=\\cos (\\theta)=\\frac{p \\cdot q }{\\|p\\|\\|q\\|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6VfNVJTniso"
      },
      "outputs": [],
      "source": [
        "def cosSimilarity(p, q):\n",
        "    score = np.dot(p,q) / (np.linalg.norm(p) * np.linalg.norm(q))\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D15_YJKXnist"
      },
      "source": [
        "2. Implement a function that predicts the class of a validation instance using the k-NN algorithm. The function should take a validation instance and the parameter $k$ as input, and output predicted class of the validation instance (+1 or -1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKh9Ze2Znisw"
      },
      "outputs": [],
      "source": [
        "def predict(x, k):  # x is the input, k is the number of neighbours\n",
        "    # The first thing we do is computing the similarity scores between x and each instance z in our train dataset.\n",
        "    # We must also store the labels so that we can later find the majority label.\n",
        "    L = [(y, cosSimilarity(x, z)) for (y,z) in train_data]\n",
        "\n",
        "    # Next, we need to find the neighbours.\n",
        "    # For that we sort this list of tuples by the value of the second item in tuples, which is similarity.\n",
        "    #\n",
        "    # We use lambda expressions, which are convenient for writing in-place functions.\n",
        "    # Here, we take an element from our list and return its similarity component,\n",
        "    # which is used by the sort function to compare two elements.\n",
        "    # \"reverse=True\" ensures that sorting is done in the descending order of similarity.\n",
        "    L.sort(key=lambda a: a[1], reverse=True)\n",
        "\n",
        "    # If you would like to confirm that it is indeed the descending order you can print the list after sorting (uncomment the line below).\n",
        "    #print(L[:k])\n",
        "\n",
        "    # Next, we must find the majority label.\n",
        "    # Since we are doing binary classification and our labels are -1 and +1,\n",
        "    # when we add the labels for the nearest neigbours if we get a positive value then there must be more +1s than -1s, and vice versa.\n",
        "    # You might have to do more complicated stuff for finding the majority label if there were more than 2 classes.\n",
        "    # But it is easy for the binary case as shown here.\n",
        "    score = sum([e[0] for e in L[:k]])\n",
        "    if score > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtUtxv2YBIFf"
      },
      "source": [
        "## Step 3. Evaluate the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZN_1AEYBIFf"
      },
      "source": [
        "1. Implement ``kNNaccuracy`` function that takes the parameter ``k`` and the validation dataset as input and output the accuracy of the k-NN algorithm on the validation dataset. Use the function to compute the accuracy of prediciton of the k-NN classifier on the validation dataset, when ``k = 5``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40dffWDgnitG"
      },
      "outputs": [],
      "source": [
        "def kNNaccuracy(k, validation_data):\n",
        "    correctPredictions = 0\n",
        "    for (y,x) in validation_data:  # for each validation sample\n",
        "        if y == predict(x, k):  # get the prediction and compare with the correct label\n",
        "            correctPredictions += 1\n",
        "    accuracy = float(correctPredictions) / float(len(validation_data))\n",
        "    return accuracy\n",
        "\n",
        "print(\"Accuracy of 5-NN: \", kNNaccuracy(5, validation_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv5_0kzMBIFg"
      },
      "source": [
        "2. Now, let's compute accuracies of k-NN for all odd ``k`` from 1 to 99. Plotting k-NN accuracy versus ``k``, what is the best value of ``k`` for the validation dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoPDVen-BIFg"
      },
      "outputs": [],
      "source": [
        "kRange = range(1, 100, 2)\n",
        "# Compute accuracies\n",
        "accuracyValues = [kNNaccuracy(i, validation_data) for i in kRange]\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(kRange, accuracyValues, 'b')\n",
        "plt.xlabel(\"Parameter k\")\n",
        "plt.ylabel(\"Accuracy of k-NN\")\n",
        "plt.title(\"k-NN accuracy versus k\")\n",
        "plt.show()\n",
        "\n",
        "# Find the best k for the current validation dataset\n",
        "print(\"Best k: \", kRange[np.argmax(accuracyValues)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kNN using SKLearn\n",
        "\n",
        "Although we implemented kNN from scratch, ``SKLearn`` has its own implementation of the method, which can be checked [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
        "\n",
        "Below we just separate the label from the data."
      ],
      "metadata": {
        "id": "ndknkC0jdk_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels and instances into separate vectors\n",
        "train_labels = [item[0] for item in train_data]\n",
        "train_instances = [item[1] for item in train_data]\n",
        "\n",
        "print(\"Train Labels:\", train_labels)\n",
        "print(\"Train Instances:\", train_instances)\n",
        "\n",
        "val_labels = [item[0] for item in validation_data]\n",
        "val_instances = [item[1] for item in validation_data]\n",
        "\n",
        "print(\"Validation Labels:\", val_labels)\n",
        "print(\"Validation Instances:\", val_instances)"
      ],
      "metadata": {
        "id": "HCoZK1UreqEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can send the data and label to the kNN classifier."
      ],
      "metadata": {
        "id": "BxRwxzYRe7PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=5)  # here we are using the euclidean distance, not the cosine\n",
        "\n",
        "neigh.fit(train_instances, train_labels)"
      ],
      "metadata": {
        "id": "3vQ9zqg6dy7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can predict and calculate the accuracy"
      ],
      "metadata": {
        "id": "6PGUB_iwfIGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = neigh.predict(val_instances)  # prediction\n",
        "\n",
        "correctPredictions = 0\n",
        "for real_label, pred_label in zip(val_labels, y_pred):\n",
        "    if real_label == pred_label:\n",
        "        correctPredictions += 1\n",
        "accuracy = float(correctPredictions) / float(len(val_labels))\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "rqxG2CxOfWks"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}