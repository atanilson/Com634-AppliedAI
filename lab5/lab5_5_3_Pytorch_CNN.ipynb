{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNVF6cNEVVe0"
      },
      "source": [
        "# 5-3  Fashion MNIST classification\n",
        "\n",
        "We still use the Fashion MNIST dataset to do classification with Convolutional layers, Batch normalization and MaxPooling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kleN4T5fVVe3"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "In this specific case, we will NOT define a dataloader as we did before, because we will process image by image, not in batches."
      ],
      "metadata": {
        "id": "iWHrC5WdYUue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PUt921HN2LX"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "train_set = torchvision.datasets.FashionMNIST(root = './data/FashionMNIST', download = True,\n",
        "                                              train = True, transform = transforms.Compose([transforms.ToTensor(),]))\n",
        "test_set = torchvision.datasets.FashionMNIST(root = './data/FashionMNIST', download=True,\n",
        "                                             train=False, transform = transforms.Compose([transforms.ToTensor()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0gqjzvwMRhL"
      },
      "source": [
        "## Define network\n",
        "\n",
        "Let's first create a CNN architecture and then explain the layers.\n",
        "\n",
        "We first define our CNN Class by inheriting **nn.Module**, and then create each layer of CNN in **init**. All operations of the neural network are implemented through the forward function. In this CNN example, there are two 2-dimensional convolutional layers, 2 Batch Normalization, 1 Max Pooling, and 2 fully connected linear layers, which are connected through some activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CxkkLL8Nfyi"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1,10,5)    # Convolutional layer\n",
        "        self.bn1 = nn.BatchNorm2d(10)     # batch normalization\n",
        "        self.mp1 = nn.MaxPool2d(2, 2)     # Max Pooling\n",
        "\n",
        "        self.conv2 = nn.Conv2d(10,20,3)\n",
        "        self.bn2 = nn.BatchNorm2d(20)\n",
        "\n",
        "        self.fc1 = nn.Linear(20*10*10,500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_size = x.size(0)\n",
        "        # in: batch*1*28*28, out: batch*10*24*24  -- (28-5+1)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)  # does not change the input size\n",
        "        x = self.bn1(x)  # does not change the input size\n",
        "        # in: batch*10*24*24, out: batch*10*12*12\n",
        "        x = self.mp1(x)\n",
        "\n",
        "        # in: batch*10*12*12, out: batch*20*10*10  -- (12-3+1)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # 20*10*10 = 2000\n",
        "        x = x.view(input_size,-1)\n",
        "\n",
        "        # in: batch*2000  out:batch*500\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # in:batch*500 out:batch*10\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGcGeoz0f84T"
      },
      "source": [
        "### Convolutional layer\n",
        "Fashionmnist is a two-dimensional image dataset to be recognized, so we use two-dimensional convolutional layer **torch.nn.Conv2d**.\n",
        "\n",
        "`torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                stride=1, padding=0, dilation=1, groups=1,\n",
        "                bias=True, padding_mode='zeros')`\n",
        "\n",
        "- in_channels (int): number of input image channels  \n",
        "- out_channels (int): the number of channels after convolution  \n",
        "- kernel_size (int or tuple): convolution kernel size  \n",
        "- stride (int, optional): Convolution stride, default is 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oooOc7K6hLHk"
      },
      "source": [
        "Let's look at an example operation of convolution layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K9C5rYzgqKw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "input = torch.randn(1,1,28,28)\n",
        "conv1 = nn.Conv2d(1,10,5)\n",
        "output = conv1(input)\n",
        "\n",
        "print(input.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06LhR6QihkPI"
      },
      "source": [
        "The size of input image is `(1x1x28x28)`:\n",
        "* The first 1 is the batch size, which can be ignored here.\n",
        "* The second is number of channel, for 1X28x28 image. The input of the convolutional layer is also a single channel, which needs to be consistent with the number of channels of the image! The output is 10 channels, and the size of the convolution kernel is 5x5. So our output is naturally (1x10x24x24): where batch size = 1 remains unchanged, the image becomes **(24x24), 24 = 28 - 5 + 1**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI2GykI_mTMi"
      },
      "source": [
        "### Batch Normalization\n",
        "\n",
        "Batch Normalization (BN) is a technique used to normalize activations within a neural network, improving **training speed** and **stability**. It helps reduce internal covariate shift by normalizing the input of each layer across a mini-batch.\n",
        "\n",
        "For each feature in a mini-batch:\n",
        "\n",
        "1. Compute the mean $\\mu$ and variance $\\sigma$ of the feature\n",
        "2. Normalize the feature:\n",
        "\n",
        "$$\\hat{x} = \\frac{x-\\mu}{\\sigma}$$\n",
        "\n",
        "3. Scale and shift using learnable parameters $\\gamma$ (scale) and $\\beta$ (shift):\n",
        "\n",
        "$$y = \\gamma\\hat{x} + \\beta$$\n",
        "\n",
        "These parameters allow the model to adjust the normalized values if necessary.\n",
        "\n",
        "In PyTorch, we can use Batch Normalization using this method:\n",
        "\n",
        "```python\n",
        "torch.nn.BatchNorm2d(num_features)\n",
        "```\n",
        "\n",
        "where, `num_features` represents the number of features that you have, i.e., the number of channels or neurons of the previous layers.\n",
        "\n",
        "Observe that, the parameters of the Batch Normalization are learned during the training ONLY. Since the parameters are fixed after the network training is completed, the mean and variance of each validation/testing batch are unchanged. But how to do this in Pytorch?\n",
        "\n",
        "In Pytorch, to change this we can just use `.train()`, to train the related parameters, such as the ones in the Batch Normalization. If we want to preserve the parameters, we can use `.eval()`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network = CNN()  # network defined before\n",
        "print(network)\n",
        "\n",
        "network.train()  # use this to train the parameters\n",
        "\n",
        "network.eval()  # use this during validation or testing to avoid learning parameters"
      ],
      "metadata": {
        "id": "TBcHAeflcmxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR1_-6Ckr0CP"
      },
      "source": [
        "### Max Pooling\n",
        "The pooling layer is used to downsample/compress image(matrix), thereby reducing network computing consumption and redundancy. There are some operators based on [Pytorch](https://pytorch.org/docs/stable/nn.html#pooling-layers), such as the MaxPool2D:\n",
        "\n",
        "\n",
        "```python\n",
        "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)\n",
        "```\n",
        "\n",
        "Here is an example of `MaxPool`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bmh3W-0sCzR"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import MaxPool2d\n",
        "\n",
        "input_5x5 = torch.tensor([[1, 2, 0, 3, 1],\n",
        "                          [0, 1, 2, 3, 1],\n",
        "                          [1, 2, 1, 0, 0],\n",
        "                          [5, 2, 3, 1, 1],\n",
        "                          [2, 1, 0, 1, 1]], dtype=torch.float32)\n",
        "input_5x5 = torch.reshape(input_5x5, (-1, 1, 5, 5))\n",
        "print(\"Input Size: \", input_5x5.shape)\n",
        "\n",
        "class Test(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Test, self).__init__()\n",
        "        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=True)  # 3x3\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = self.maxpool1(input_data)\n",
        "        return output\n",
        "\n",
        "test = Test()\n",
        "output = test(input_5x5)\n",
        "print(\"Output: \", output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNPU7xgdt1jT"
      },
      "source": [
        "Using a Maxpool layer of 3x3, a 5x5 matrix become 2x2.\n",
        "\n",
        "The relationship of input size and output size can be found: https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it all together\n",
        "\n",
        "After understanding the operations, let's understand the most basic usage: after creating a Net class, we can directly enter the input, and the result can be obtained through the forward function.\n",
        "Of course, this example is just a simple demonstration, the model has not been trained, so the output is not accurate every time."
      ],
      "metadata": {
        "id": "qppOJVIBboaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eVWqgtMh4j8"
      },
      "outputs": [],
      "source": [
        "network = CNN()  # network defined before\n",
        "print(network)\n",
        "\n",
        "print('input', input.shape)\n",
        "output = network(input)\n",
        "pred = F.softmax(output, dim=1).argmax(1)\n",
        "print(pred)  # prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVwDjGHRkTF3"
      },
      "source": [
        "## Hyperparameters and Optimizer\n",
        "\n",
        "Let's define some.\n",
        "\n",
        "We also know that the stochastic gradient descent algorithm converges faster when the learning rate becomes larger. In this way, we do not need to adjust the learning rate tediously, which greatly improves the efficiency of the optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMHIRyOOkXf2"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 60\n",
        "epochs = 5\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate) # Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ91XAYOlN_8"
      },
      "source": [
        "In a training loop, the optimization has 3 steps:\n",
        "\n",
        "1. Execute optimizer.zero_grad to clear the gradient accumulated in the system,\n",
        "2. The prediction loss is backpropagated by calling loss.backward(). PyTorch will store the loss gradient corresponding to each parameter.\n",
        "3. After getting the loss gradient, call optimizer.step() to optimize and adjust parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piGJdgfYlXwo"
      },
      "outputs": [],
      "source": [
        "def train(network, train_loader, optimizer):\n",
        "    network.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = network(data)\n",
        "        loss = loss_function(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIz6Pcljm_93"
      },
      "source": [
        "This implements the validation/testing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIolnPdYm30B"
      },
      "outputs": [],
      "source": [
        "def accuracy(epoch_idx, test_loader, network, set_type = None):\n",
        "    network.eval()\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():      # to calculate accuracy, we do not need the gradient any more\n",
        "        for data, target in test_loader:\n",
        "            outputs = network(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    if set_type == \"train\":\n",
        "        print('\\nEpoch{}: Train accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch_idx, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    if set_type == \"test\":\n",
        "        print('\\nEpoch{}: Test accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            epoch_idx, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    return correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrCUPcMHnipL"
      },
      "source": [
        "## Training\n",
        "In our example, we only train 5 epochs by a simple CNN network. Using complex CNN structures, such as VGG, ResNet, etc., and more epochs are ways to improve accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVu887_6niDI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set,batch_size=104,shuffle=True) # training set shuffle the data\n",
        "test_loader = DataLoader(dataset=test_set,batch_size=5,shuffle=False) # testing set fix the data order\n",
        "\n",
        "network = CNN()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(1,epochs+1):\n",
        "  print(f\"Epoch {i}\\n-------------------------------\")\n",
        "  train(network = network, train_loader = train_loader, optimizer = optimizer)\n",
        "  train_accuracy = accuracy(epoch_idx=i, test_loader = train_loader, network = network, set_type = \"train\")\n",
        "  val_accuracy = accuracy(epoch_idx=i, test_loader = test_loader, network = network, set_type = \"test\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}