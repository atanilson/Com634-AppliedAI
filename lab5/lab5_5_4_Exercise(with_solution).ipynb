{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHLHPnMFVVjM"
      },
      "source": [
        "# 5-4 Exercise\n",
        "\n",
        "According Lab session `5-1` to `5-3`, we can do some exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Firstly, you can change the linear function from Session 5.1 to a nonlinear function, like x^2, x^3...; Then using (mini) Batch Gradient Descent or Stochastic Gradient Descend (SGD), to check the loss curve."
      ],
      "metadata": {
        "id": "BN8vZXlJoUsd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfv4nVBTVVjO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# produce the data point with linear function\n",
        "X = torch.arange(-5, 5, 0.05).view(-1, 1)\n",
        "func = X**2\n",
        "# Gaussian noise is added to create the variable Y\n",
        "Y = func + 0.2 * torch.randn(X.size())\n",
        "\n",
        "# plot and visualize the data points\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "\n",
        "ax1.plot(X, Y, 'b*', label='Y')\n",
        "ax1.plot(X, func, 'r', label='function')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.legend()\n",
        "ax1.grid('True', color='y')\n",
        "\n",
        "# define the forward function\n",
        "def forward(x):\n",
        "    return w * x + b\n",
        "\n",
        "# define loss function with Mean Square Error (MSE)\n",
        "def criterion(y_pred, y):\n",
        "    return torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "#  initial parameters w and b\n",
        "w = torch.tensor(-10.0, requires_grad=True)\n",
        "b = torch.tensor(-20.0, requires_grad=True)\n",
        "\n",
        "#  other parameters\n",
        "step_size = 0.1\n",
        "loss_BGD = []\n",
        "n_iter = 100\n",
        "\n",
        "#Initial predictions\n",
        "print('Predict before training with BGD: x=' + str(4) + ' y=' + str(4**2) + ' prediction=' + str(forward(4.0)))\n",
        "\n",
        "for i in range (n_iter):\n",
        "    # making predictions with forward pass\n",
        "    Y_pred = forward(X)\n",
        "    # calculating the loss between original and predicted data points\n",
        "    loss = criterion(Y_pred, Y)\n",
        "    # storing the calculated loss in a list\n",
        "    loss_BGD.append(loss.item())\n",
        "    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
        "    loss.backward()\n",
        "    # updateing the parameters after each iteration\n",
        "    w.data = w.data - step_size * w.grad.data\n",
        "    b.data = b.data - step_size * b.grad.data\n",
        "    # zeroing gradients after each iteration\n",
        "    w.grad.data.zero_()\n",
        "    b.grad.data.zero_()\n",
        "    # priting some values for understanding\n",
        "    if i % 5 == 0:\n",
        "        print('iteration: {}, \\t loss: {}, \\t weight: {}, \\t bias: {}'.format(i, loss.item(), w.item(), b.item()))\n",
        "\n",
        "#Predict y after updating w\n",
        "print('Predict after training with BGD: x=' + str(4) + ' y=' + str(4**2) + ' prediction=' + str(forward(4.0)))\n",
        "\n",
        "# plot the figure (loss_BGD)\n",
        "plt.plot(loss_BGD, label=\"Batch Gradient Descent\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost/Total loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvHDF9FRVVjT"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Secondly, search the learning rate between (0, 1), and find the best learning rate for the Session 5.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9yOAhs1VVjV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define Hyperparameters\n",
        "# set img_size = (28,28) ---> 28*28=784 pixels in total\n",
        "input_size = 784\n",
        "# number of nodes at hidden layer\n",
        "hidden_size = 500\n",
        "# number of output classes discrete range [0,9]\n",
        "num_classes = 10\n",
        "\n",
        "# number of times which the entire dataset is passed throughout the model\n",
        "num_epochs = 30\n",
        "\n",
        "# the size of input data took for one iteration\n",
        "batch_size = 1000\n",
        "\n",
        "# loss function\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84k9xeE9VVjX"
      },
      "outputs": [],
      "source": [
        "# Download MNIST data\n",
        "train_data = datasets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = datasets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "# Split dataset with DataLoader, train dataset and test dataset\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle = False)\n",
        "\n",
        "# Use GPU, if the GPU is available, otherwise use the CPU.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x_train,y_train = next(iter(train_gen))\n",
        "print(x_train.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im8z3SCpVVjZ"
      },
      "outputs": [],
      "source": [
        "# Define neural network model\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()  # Relu activation function, you can also use others like Tanh, Sigmold, etc.\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "# Build the model of neural network\n",
        "net = Net(input_size, hidden_size, num_classes)\n",
        "# feed net to device\n",
        "net.to(device)\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines the learning rate values to be tested."
      ],
      "metadata": {
        "id": "DyW3sJ_op7O8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYlLBAskVVjb"
      },
      "outputs": [],
      "source": [
        "lr_list = np.arange(0.0001, 0.01, 0.001)\n",
        "print(lr_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KFBZLfPVVjc"
      },
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Lists for knowing classwise accuracy\n",
        "predicted_list = []\n",
        "labels_list = []\n",
        "\n",
        "# set learning rate as iteration number\n",
        "for lr in lr_list:\n",
        "    # Adam optimizer, you can also use AdaGrad or RMSProp, etc.\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)   # current learning rate\n",
        "\n",
        "    # train the model\n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "      for i ,(images,labels) in enumerate(train_gen):\n",
        "          images = Variable(images.view(-1,28*28))\n",
        "          # if you have GPU, you can set as  .cuda()\n",
        "          # images = Variable(images.view(-1,28*28)).cuda()\n",
        "          # if you have GPU, you can set as  .cuda()\n",
        "          # labels = Variable(labels).cuda()\n",
        "          labels = Variable(labels)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = net(images)\n",
        "          loss = loss_function(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print('Step [%d/%d], Loss: %.4f'\n",
        "                      %( i+1, len(train_data)//batch_size, loss.item()))\n",
        "    loss_list.append(loss.data)\n",
        "\n",
        "    # Evaluate the accuracy of the model\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    for images, labels in test_gen:\n",
        "        # if you have GPU, you can set as  .cuda()\n",
        "        # images = Variable(images.view(-1,28*28)).cuda()\n",
        "        images = Variable(images.view(-1,28*28))\n",
        "        # labels = labels.cuda()\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        output = net(images)\n",
        "\n",
        "        _, predicted = torch.max(output,1)\n",
        "        predicted_list.append(predicted)\n",
        "\n",
        "        correct += (predicted == labels).sum()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = (100*correct)/(total)\n",
        "    # loss_list.append(loss.data)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "    print(\"Learning Rate: {}, Loss: {}, Accuracy: {}%\".format(lr, loss.data, accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(len(lr_list))"
      ],
      "metadata": {
        "id": "vKjKL_45tXWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBUC3ZWpVVjg"
      },
      "outputs": [],
      "source": [
        "plt.bar(np.arange(len(lr_list)), accuracy_list)\n",
        "plt.xlabel(\"LR value\")\n",
        "plt.xticks(np.arange(len(lr_list)), lr_list, rotation=65)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(97,99)\n",
        "plt.title(\"LR vs Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DvUNCwaVVjh"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Lastly, using the Fashion MNIST dataset, how can you improve the accuracy?\n",
        "Please explore any method to achieve a much higher accuracy. For example, you can change the network model by add more layers, tune hyperparameters, or any other ideas to obtain a better results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP1M_IveVVji"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El45hXyOVVjj"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "train_set = torchvision.datasets.FashionMNIST(root = './data/FashionMNIST', download = True,\n",
        "                                              train = True, transform = transforms.Compose([transforms.ToTensor(),]))\n",
        "test_set = torchvision.datasets.FashionMNIST(root = './data/FashionMNIST', download=True,\n",
        "                                             train=False, transform = transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "# split to train loader\n",
        "train_loader = DataLoader(dataset=train_set,batch_size=100,shuffle=True) # training set shuffle the data\n",
        "test_loader = DataLoader(dataset=test_set,batch_size=50,shuffle=False) # testing set fix the data order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FS9Z1TmVVjk"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1,10,5)    # Convolutional layer\n",
        "        self.conv2 = nn.Conv2d(10,20,3)\n",
        "\n",
        "        self.fc1 = nn.Linear(20*10*10,500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_size = x.size(0)\n",
        "        # in: batch*1*28*28, out: batch*10*24*24(28-5+1)\n",
        "        x = self.conv1(x)\n",
        "        # out: batch*10*24*24\n",
        "        x = F.relu(x)\n",
        "        # in: batch*10*24*24, out: batch*10*12*12\n",
        "        x = F.max_pool2d(x,2,2)\n",
        "\n",
        "        # in: batch*10*12*12, out: batch*20*10*10 (12-3+1)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # 20*10*10 = 2000\n",
        "        x = x.view(input_size,-1)\n",
        "\n",
        "        # in: batch*2000  out:batch*500\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # in:batch*500 out:batch*10\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# create the object for model CNN\n",
        "net= CNN()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUuXSaQSVVjl"
      },
      "outputs": [],
      "source": [
        "# define the parameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4gzMbffVVjn"
      },
      "outputs": [],
      "source": [
        "#  train the model\n",
        "correct = 0\n",
        "loss_list = []\n",
        "for i in range(1,epochs+1):\n",
        "   net.train()\n",
        "   for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = net(data)\n",
        "\n",
        "      loss = loss_function(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "   loss_list.append(loss.data)\n",
        "   print('Epoch: {}, \\t loss: {}'.format(i, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsn8i6EPVVjo"
      },
      "outputs": [],
      "source": [
        "# plot the figure\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.plot(loss_list)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1vKoLzsVVjo"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for data, target in test_loader:\n",
        "    outputs = net(data)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    correct += (predicted == target).sum().item()\n",
        "\n",
        "print('Test accuracy: {}/{} ({:.2f}%)\\n'.format(correct, len(test_loader.dataset),\n",
        "                                                100. * correct / len(test_loader.dataset)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}