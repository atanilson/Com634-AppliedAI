{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QXAxyA1usb5"
      },
      "source": [
        "# 5-2 An example\n",
        "\n",
        "Hyperparameters are tunable in training a neural network, you can change the hyperprameters to find the best performance.\n",
        "\n",
        "In neural networks, there are several hyperparameters that you can tune, like:\n",
        "- Number of Hidden Layers\n",
        "- Number of Neurons per Hidden Layers\n",
        "- Learning Rate\n",
        "- Batch Size\n",
        "- Optimizer.\n",
        "\n",
        "For example, you can change the value of Learning Rate to see what your training will be.\n",
        "\n",
        "In this section, we just tune one parameter once. You can tune several parameters from a parameter search space using techniques such as [Grid Search](https://machinelearningmastery.com/how-to-grid-search-hyperparameters-for-pytorch-models/), [Random Search](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/), and [ray tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mArIZCL6uscC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define hyperparameters\n",
        "\n",
        "It is important to tune these parameters so that you can extract the most possible from the models."
      ],
      "metadata": {
        "id": "0WI0FwOwTlLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bNfVLRUYqZA"
      },
      "outputs": [],
      "source": [
        "# set img_size = (28,28) ---> 28*28=784 pixels in total\n",
        "input_size = 784\n",
        "\n",
        "# number of nodes at hidden layer\n",
        "hidden_size = 500\n",
        "\n",
        "# number of output classes discrete range [0,9]\n",
        "num_classes = 10\n",
        "\n",
        "# number of times which the entire dataset is passed throughout the model\n",
        "num_epochs = 30\n",
        "\n",
        "# the size of input data took for one iteration\n",
        "batch_size = 100\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data"
      ],
      "metadata": {
        "id": "gCd6LdX1TnoN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCsBCXMwbpH5"
      },
      "outputs": [],
      "source": [
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfDPBdnYgfGp"
      },
      "outputs": [],
      "source": [
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                        batch_size = batch_size,\n",
        "                                        shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "sqa1YuaVTova"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL-YXTvghaz_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()  # Relu activation function, you can also use others like Tanh, Sigmold, etc.\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3EPEqbjjfAT"
      },
      "outputs": [],
      "source": [
        "net = Net(input_size, hidden_size, num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define loss-function & optimizer"
      ],
      "metadata": {
        "id": "d-rcALv1Tyjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePLIwvAFj2zH"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam optimizer -- you can also use SGD, AdaGrad or RMSProp, etc.\n",
        "optimizer = torch.optim.Adam( net.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "M0P2ZAsBT1DF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u75Xa5VckuTH"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for i ,(images,labels) in enumerate(train_gen):\n",
        "    # if you have GPU, you can set as  .cuda()\n",
        "    images = Variable(images.view(-1,28*28)).cuda()\n",
        "    # otherwise, remove the .cuda(), as below\n",
        "    # images = Variable(images.view(-1,28*28))\n",
        "\n",
        "    # if you have GPU, you can set as  .cuda()\n",
        "    labels = Variable(labels).cuda()\n",
        "    # otherwise, remove the .cuda(), as below\n",
        "    # labels = Variable(labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the accuracy of the model"
      ],
      "metadata": {
        "id": "dOh7s-S4UhuD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTPvMW5jHB9X"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for images,labels in test_gen:\n",
        "  # if you have GPU, you can set as  .cuda()\n",
        "  images = Variable(images.view(-1,28*28)).cuda()\n",
        "  # otherwise, remove the .cuda(), as below\n",
        "  # images = Variable(images.view(-1,28*28))\n",
        "\n",
        "  # labels = labels.cuda()\n",
        "  labels = labels\n",
        "\n",
        "  output = net(images)\n",
        "  _, predicted = torch.max(output,1)\n",
        "  correct += (predicted.cpu().numpy() == labels).sum()  # .cpu() tranfers the data from GPU to CPU, and .numpy() converts the data from torch to numpy\n",
        "  total += labels.size(0)\n",
        "\n",
        "print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQg1JkJDuscV"
      },
      "source": [
        "When **num_epochs** = 30, **learning rate** = 1e-3, and **batch size** = 100, the result is around 98%. When we change the **num_epochs** and the other hyperparameters, we can obatain other results. We can iterate the parameters and plot the best results.\n",
        "\n",
        "This is left as an exercise for you, including the plot."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}