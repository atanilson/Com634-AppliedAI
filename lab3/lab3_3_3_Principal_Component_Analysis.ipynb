{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8_BFY_ctaO"
      },
      "source": [
        "#  3. Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIE77JvpctaO"
      },
      "source": [
        "In this section, we explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA).\n",
        "PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.\n",
        "After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRLRd_AcctaP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set_theme()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_PGC_-NctaQ"
      },
      "source": [
        "## Introducing Principal Component Analysis\n",
        "\n",
        "Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data.\n",
        "Its behavior is easiest to visualize by looking at a two-dimensional dataset.\n",
        "Consider the following 200 points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNTwZbcoctaQ"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqBCdQp_ctaR"
      },
      "source": [
        "By eye, it is clear that there is a nearly linear relationship between the x and y variables.\n",
        "This is reminiscent of the linear regression data we explored in Lab Session2: 2.5-Linear Regression and Logistic Function, but the problem setting here is slightly different: rather than attempting to *predict* the y values from the x values, the unsupervised learning problem attempts to learn about the *relationship* between the x and y values.\n",
        "\n",
        "In principal component analysis, this relationship is quantified by finding a list of the *principal axes* in the data, and using those axes to describe the dataset.\n",
        "Using Scikit-Learn's ``PCA`` estimator, we can compute this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl7POOO0ctaR"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6I0NXbYctaR"
      },
      "source": [
        "The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzL4_wb_ctaR"
      },
      "outputs": [],
      "source": [
        "print(pca.components_)  # eigenvectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3LaDFiLctaS"
      },
      "outputs": [],
      "source": [
        "print(pca.explained_variance_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCh0Bk1nctaS"
      },
      "source": [
        "To see what these numbers mean, let's visualize them as vectors over the input data, using the \"components\" to define the direction of the vector, and the \"explained variance\" to define the squared-length of the vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvl4Xs36ctaS"
      },
      "outputs": [],
      "source": [
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->',\n",
        "                    linewidth=2,\n",
        "                    shrinkA=0, shrinkB=0, color='black')\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops, color=\"black\")\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n--uKSW_ctaS"
      },
      "source": [
        "These vectors represent the *principal axes* of the data, and the length of the vector is an indication of how \"important\" that axis is in describing the distribution of the dataâ€”more precisely, it is a measure of the variance of the data when projected onto that axis.\n",
        "The projection of each data point onto the principal axes are the \"principal components\" of the data.\n",
        "\n",
        "If we plot these principal components beside the original data, we see the plots shown here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiI4civMctaS"
      },
      "source": [
        "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.09-PCA-rotation.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLO1fJEhctaS"
      },
      "source": [
        "This transformation from data axes to principal axes is an *affine transformation*, which basically means it is composed of a translation, rotation, and uniform scaling.\n",
        "\n",
        "While this algorithm to find principal components may seem like just a mathematical curiosity, it turns out to have very far-reaching applications in the world of machine learning and data exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34mp0XC-ctaS"
      },
      "source": [
        "### PCA as dimensionality reduction\n",
        "\n",
        "Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.\n",
        "\n",
        "Here is an example of using PCA as a dimensionality reduction transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a0aqMjGctaS"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(\"original shape:   \", X.shape)\n",
        "print(\"transformed shape:\", X_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7SZurMctaT"
      },
      "source": [
        "The transformed data has been reduced to a single dimension.\n",
        "To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbxlyv2NctaT"
      },
      "outputs": [],
      "source": [
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)  # the original data - blue data points\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)  # the retrieved (via inverse) data - reddish data points\n",
        "plt.axis('equal');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmw_kfBzctaT"
      },
      "source": [
        "The light points are the original data, while the solid points are the projected version.\n",
        "This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.\n",
        "The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality.\n",
        "\n",
        "This reduced-dimension dataset is in some senses \"good enough\" to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}