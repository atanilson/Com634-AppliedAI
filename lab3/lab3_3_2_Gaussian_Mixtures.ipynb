{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd2ETBIkZPMj"
      },
      "source": [
        "# 2. Gaussian Mixture Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nWP78dZZPMj"
      },
      "source": [
        "The *k*-means clustering model explored in the previous section is simple and relatively easy to understand, but its simplicity leads to practical challenges in its application.\n",
        "In particular, the non-probabilistic nature of *k*-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations.\n",
        "In this section we will take a look at Gaussian mixture models (GMMs), which can be viewed as an extension of the ideas behind *k*-means, but can also be a powerful tool for estimation beyond simple clustering.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM5HR1sGZPMk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set_theme()\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ObXmjAZPMk"
      },
      "source": [
        "## Motivating GMM: Weaknesses of k-Means\n",
        "\n",
        "Let's take a look at some of the weaknesses of *k*-means and think about how we might improve the cluster model.\n",
        "As we saw in the previous section, given simple, well-separated data, *k*-means finds suitable clustering results.\n",
        "\n",
        "For example, if we have simple blobs of data, the *k*-means algorithm can quickly label those clusters in a way that closely matches what we might do by eye:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLhzareoZPMl"
      },
      "outputs": [],
      "source": [
        "# Generate some data\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=400, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "X = X[:, ::-1] # flip axes for better plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_mH_U8ZPMl"
      },
      "outputs": [],
      "source": [
        "# Plot the data with K Means Labels\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(4, random_state=0)\n",
        "labels = kmeans.fit(X).predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgc5GlRaZPMm"
      },
      "source": [
        "From an intuitive standpoint, we might expect that the clustering assignment for some points is more certain than others: for example, there appears to be a very slight overlap between the two middle clusters, such that we might not have complete confidence in the cluster assigment of points between them.\n",
        "Unfortunately, the *k*-means model has no intrinsic measure of probability or uncertainty of cluster assignments (although it may be possible to use a bootstrap approach to estimate this uncertainty).\n",
        "For this, we must think about generalizing the model.\n",
        "\n",
        "One way to think about the *k*-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster.\n",
        "This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster.\n",
        "We can visualize this cluster model with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rARzcSuBZPMm"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n",
        "    labels = kmeans.fit_predict(X)\n",
        "\n",
        "    # plot the input data\n",
        "    ax = ax or plt.gca()\n",
        "    ax.axis('equal')\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
        "\n",
        "    # plot the representation of the KMeans model\n",
        "    centers = kmeans.cluster_centers_\n",
        "    radii = [cdist(X[labels == i], [center]).max()\n",
        "             for i, center in enumerate(centers)]\n",
        "    for c, r in zip(centers, radii):\n",
        "        ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnSIK-piZPMm"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "plot_kmeans(kmeans, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1W0ZLPFZPMm"
      },
      "source": [
        "An important observation for *k*-means is that these cluster models **must be circular**: *k*-means has no built-in way of accounting for oblong or elliptical clusters.\n",
        "So, for example, if we take the same data and transform it, the cluster assignments end up becoming muddled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHZ4DtFdZPMm"
      },
      "outputs": [],
      "source": [
        "rng = np.random.RandomState(13)\n",
        "X_stretched = np.dot(X, rng.randn(2, 2))\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "plot_kmeans(kmeans, X_stretched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo4wDOHKZPMm"
      },
      "source": [
        "By eye, we recognize that these transformed clusters are non-circular, and thus circular clusters would be a poor fit.\n",
        "Nevertheless, *k*-means is not flexible enough to account for this, and tries to force-fit the data into four circular clusters.\n",
        "This results in a mixing of cluster assignments where the resulting circles overlap: see especially the bottom-right of this plot.\n",
        "One might imagine addressing this particular situation by preprocessing the data with PCA, but in practice there is no guarantee that such a global operation will circularize the individual data.\n",
        "\n",
        "These two disadvantages of *k*-means—its lack of flexibility in cluster shape and lack of probabilistic cluster assignment—mean that for many datasets (especially low-dimensional datasets) it may not perform as well as you might hope.\n",
        "\n",
        "You might imagine addressing these weaknesses by generalizing the *k*-means model: for example, you could measure uncertainty in cluster assignment by comparing the distances of each point to *all* cluster centers, rather than focusing on just the closest.\n",
        "You might also imagine allowing the cluster boundaries to be ellipses rather than circles, so as to account for non-circular clusters.\n",
        "It turns out these are two essential components of a different type of clustering model, Gaussian mixture models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0zPfnmWZPMn"
      },
      "source": [
        "## Generalizing E–M: Gaussian Mixture Models\n",
        "\n",
        "A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset.\n",
        "In the simplest case, GMMs can be used for finding clusters in the same manner as *k*-means:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlYjqejTZPMn"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "\n",
        "gmm = GMM(n_components=4).fit(X)\n",
        "labels = gmm.predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JloklI17ZPMn"
      },
      "source": [
        "But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the ``predict_proba`` method.\n",
        "This returns a matrix of size ``[n_samples, n_clusters]`` which measures the probability that any point belongs to the given cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neL_5QJSZPMn"
      },
      "outputs": [],
      "source": [
        "probs = gmm.predict_proba(X)\n",
        "print(probs[:5].round(3))  # first 5 rows x 4 clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GzJR0wMZPMn"
      },
      "source": [
        "We can visualize this uncertainty by, for example, making the size of each point proportional to the certainty of its prediction; looking at the following figure, we can see that it is precisely the points at the boundaries between clusters that reflect this uncertainty of cluster assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f47KiDroZPMn"
      },
      "outputs": [],
      "source": [
        "size = 50 * probs.max(1) ** 2  # square emphasizes differences\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2nw4r8fZPMn"
      },
      "source": [
        "Under the hood, a Gaussian mixture model is very similar to *k*-means: it uses an expectation–maximization approach which qualitatively does the following:\n",
        "\n",
        "1. Choose starting guesses for the location and shape\n",
        "\n",
        "2. Repeat until converged:\n",
        "\n",
        "   1. *E-step*: for each point, find weights encoding the probability of membership in each cluster\n",
        "   2. *M-step*: for each cluster, update its location, normalization, and shape based on *all* data points, making use of the weights\n",
        "\n",
        "The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model.\n",
        "Just as in the *k*-means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used.\n",
        "\n",
        "Let's create a function that will help us visualize the locations and shapes of the GMM clusters by drawing ellipses based on the GMM output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsmbSGmvZPMo"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
        "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Convert covariance to principal axes\n",
        "    if covariance.shape == (2, 2):\n",
        "        U, s, Vt = np.linalg.svd(covariance)\n",
        "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
        "        width, height = 2 * np.sqrt(s)\n",
        "    else:\n",
        "        angle = 0\n",
        "        width, height = 2 * np.sqrt(covariance)\n",
        "\n",
        "    # Draw the Ellipse\n",
        "    for nsig in range(1, 4):\n",
        "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
        "                             angle=angle, alpha=0.3, color=\"gray\"))\n",
        "\n",
        "def plot_gmm(gmm, X, label=True, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    labels = gmm.fit(X).predict(X)\n",
        "    if label:\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
        "    else:\n",
        "        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
        "    ax.axis('equal')\n",
        "\n",
        "    w_factor = 0.2 / gmm.weights_.max()\n",
        "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
        "        draw_ellipse(pos, covar, alpha=w * w_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljOwSiIQZPMo"
      },
      "source": [
        "With this in place, we can take a look at what the four-component GMM gives us for our initial data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kfg0Hb2ZPMo"
      },
      "outputs": [],
      "source": [
        "gmm = GMM(n_components=4, random_state=42)\n",
        "plot_gmm(gmm, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4thj-YSZZPMo"
      },
      "source": [
        "Similarly, we can use the GMM approach to fit our stretched dataset; allowing for a full covariance the model will fit even very oblong, stretched-out clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tifww8eZPMo"
      },
      "outputs": [],
      "source": [
        "gmm = GMM(n_components=4, covariance_type='full', random_state=42)\n",
        "\n",
        "plot_gmm(gmm, X_stretched)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDIYReWKZPMo"
      },
      "source": [
        "This makes clear that GMM addresses the two main practical issues with *k*-means encountered before."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}