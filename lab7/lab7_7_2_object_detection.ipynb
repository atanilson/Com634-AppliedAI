{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t0vHU8YNbFG"
      },
      "source": [
        "# Lab 7: Computer Vision Applications - Object Detection\n",
        "\n",
        "In this session, we will learn how to train a Faster RCNN model to do object detection.\n",
        "  \n",
        "Here are all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDUBNeBzEFwJ"
      },
      "outputs": [],
      "source": [
        "# linear algebra\n",
        "import numpy as np\n",
        "# data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import functional as FT\n",
        "from torchvision import transforms as T\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image\n",
        "# opencv\n",
        "import cv2\n",
        "# our data augmentation library\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TYAjs_rEUfK"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import time\n",
        "# display progress bar\n",
        "from tqdm import tqdm\n",
        "# plot bounding box\n",
        "from torchvision.utils import draw_bounding_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J12bldsiOMjF"
      },
      "source": [
        "## Dataset\n",
        "First, you need to download the Aquarium dataset in Kaggle:\n",
        "https://www.kaggle.com/datasets/sharansmenon/aquarium-dataset.\n",
        "After that, you have two options: upload it into Colab or into your Google Drive, as before.\n",
        "\n",
        "If needed, you can use the cell below to unzip the files. Adjust it if necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive.zip"
      ],
      "metadata": {
        "id": "0_S1nD6dxTU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0brY4o2N3Zi"
      },
      "source": [
        "PyCOCOTools provides many utilities for dealing with datasets in the COCO format, and if you wanted, you could evaluate the model's performance on the dataset with some of the utilities provided with this library.\n",
        "\n",
        "You may try that by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6x4-n0wEV4z"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqhdcJjROIXc"
      },
      "source": [
        "We use albumentations as our data augmentation library due to its capability to deal with bounding boxes in multiple formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfVsKwJdEbIh"
      },
      "outputs": [],
      "source": [
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbAIOS5OEcmK"
      },
      "outputs": [],
      "source": [
        "def get_transforms(train=False):\n",
        "    if train:\n",
        "        transform = A.Compose([\n",
        "            A.Resize(600, 600), # our input size can be 600px\n",
        "            A.HorizontalFlip(p=0.3),\n",
        "            A.VerticalFlip(p=0.3),\n",
        "            A.RandomBrightnessContrast(p=0.1),\n",
        "            A.ColorJitter(p=0.1),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='coco'))\n",
        "    else:\n",
        "        transform = A.Compose([\n",
        "            A.Resize(600, 600), # our input size can be 600px\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='coco'))\n",
        "    return transform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our dataset class.\n",
        "It loads all the necessary files and it processes the data so that it can be fed into the model."
      ],
      "metadata": {
        "id": "Msy79mVxxZjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aIZ7HGeEqNM"
      },
      "outputs": [],
      "source": [
        "class AquariumDetection(datasets.VisionDataset):\n",
        "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
        "        # the 3 transform parameters are reuqired for datasets.VisionDataset\n",
        "        super().__init__(root, transforms, transform, target_transform)\n",
        "        self.split = split #train, valid, test\n",
        "        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
        "\n",
        "    def _load_image(self, id: int):\n",
        "        path = self.coco.loadImgs(id)[0]['file_name']\n",
        "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        return image\n",
        "    def _load_target(self, id):\n",
        "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id = self.ids[index]\n",
        "        image = self._load_image(id)\n",
        "        target = self._load_target(id)\n",
        "        target = copy.deepcopy(self._load_target(id))\n",
        "\n",
        "        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image, bboxes=boxes)\n",
        "\n",
        "        image = transformed['image']\n",
        "        boxes = transformed['bboxes']\n",
        "\n",
        "        new_boxes = [] # convert from xywh to xyxy\n",
        "        for box in boxes:\n",
        "            xmin = box[0]\n",
        "            xmax = xmin + box[2]\n",
        "            ymin = box[1]\n",
        "            ymax = ymin + box[3]\n",
        "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
        "\n",
        "        targ = {} # here is our transformed target\n",
        "        targ['boxes'] = boxes\n",
        "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
        "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
        "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
        "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
        "        return image.div(255), targ # scale images\n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below loads the annotations so we can access the classes."
      ],
      "metadata": {
        "id": "lLZQYhXNzh4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlkh2VRCEvAy"
      },
      "outputs": [],
      "source": [
        "# load classes\n",
        "dataset_path=\"Aquarium Combined\"\n",
        "coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\n",
        "categories = coco.cats\n",
        "n_classes = len(categories.keys())\n",
        "categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZCbf2VAOgKo"
      },
      "source": [
        "This code just gets a list of classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MyAuHjHLmCA"
      },
      "outputs": [],
      "source": [
        "classes = [i[1]['name'] for i in categories.items()]\n",
        "classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the datasets!"
      ],
      "metadata": {
        "id": "VKX3GQXEzyyp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFag8HJuLtRe"
      },
      "outputs": [],
      "source": [
        "train_dataset = AquariumDetection(root=dataset_path, transforms=get_transforms(True))\n",
        "test_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnL4clkdOtqM"
      },
      "source": [
        "This is our collating function for the train dataloader, it allows us to create batches of data that can be easily pass into the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvLixUv6L5jm"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the dataloader."
      ],
      "metadata": {
        "id": "A0d--lmP1Z7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMOC9vY5L8kp"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPvB9C9sOlyz"
      },
      "source": [
        "This is a sample image and its bounding boxes, this code does not get the model's output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMIvqZkiLvZ6"
      },
      "outputs": [],
      "source": [
        "# Lets view a sample\n",
        "sample = train_dataset[2]\n",
        "img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n",
        "plt.imshow(draw_bounding_boxes(\n",
        "    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
        ").permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEfYUvm-Op80"
      },
      "source": [
        "## Model\n",
        "Our model is FasterRCNN with a backbone of `MobileNetV3-Large`.\n",
        "We need to change the output layers because we have just `7` classes but this model was trained on `80` classes from the [COCO dataset](https://cocodataset.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWupZA_rL3L0"
      },
      "outputs": [],
      "source": [
        "# Load the faster rcnn model\n",
        "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "\n",
        "# we need to change the head, get the number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUznxQ7POzXO"
      },
      "source": [
        "The following blocks ensures that the model can take in the data and that it will not crash during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87f4b4tDL-ey"
      },
      "outputs": [],
      "source": [
        "images,targets = next(iter(train_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k:v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets) # just make sure this runs without error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yvw6vPMMBf6"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") # use GPU to train\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JeJOkNTO5UQ"
      },
      "source": [
        "## Optimizer\n",
        "Here, we define the optimizer. If you wish, you can also define the `LR Scheduler`, but it is not necessary for this notebook since our dataset is so small.\n",
        "\n",
        "In this case, we do not need to define a loss function because the model has its own built-in loss. We just need to pass the labels and the model will process and return the calculated loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5IR11JJMCdM"
      },
      "outputs": [],
      "source": [
        "# parameters and optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPhRzuk-O9DC"
      },
      "source": [
        "## Training\n",
        "The following is a function that will train the model for one epoch. `Torchvision Object Detections` models have a loss function built in, and it will calculate the loss automatically if you pass in the **inputs** and **targets**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcHWaWD2MOeu"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    all_losses = []\n",
        "    all_losses_dict = []\n",
        "\n",
        "    for images, targets in tqdm(loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        all_losses.append(loss_value)\n",
        "        all_losses_dict.append(loss_dict_append)\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
        "            print(loss_dict)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
        "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
        "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
        "        all_losses_dict['loss_classifier'].mean(),\n",
        "        all_losses_dict['loss_box_reg'].mean(),\n",
        "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
        "        all_losses_dict['loss_objectness'].mean()\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3rt8hGjPKN5"
      },
      "source": [
        "20 Epochs should be enough to train this model for a high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTgSJG9BNJ6j"
      },
      "outputs": [],
      "source": [
        "num_epochs=20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFkZ8YRaPPEG"
      },
      "source": [
        "## Inference\n",
        "\n",
        "This is the inference code for the model. First, we set the model to evaluation mode and clear the GPU Cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfjdn79GPRp1"
      },
      "outputs": [],
      "source": [
        "# we will watch first epoich to ensure no errrors\n",
        "# while it is training, lets write code to see the models predictions. lets try again\n",
        "model.eval()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's get some image (in this case, image at index 20) from the test set and generate predictions using the trained model.\n",
        "\n",
        "Feel free to change the prediction image."
      ],
      "metadata": {
        "id": "8eDKxa6U4NB6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyaAzFVBPc8i"
      },
      "outputs": [],
      "source": [
        "img, _ = test_dataset[20]  # change this index here to check other images\n",
        "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])\n",
        "    pred = prediction[0]\n",
        "\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "plt.imshow(draw_bounding_boxes(img_int,\n",
        "    pred['boxes'][pred['scores'] > 0.8],\n",
        "    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=3\n",
        ").permute(1, 2, 0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J12bldsiOMjF",
        "EEfYUvm-Op80",
        "jPhRzuk-O9DC"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}