{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGU_0iONv8Rb"
      },
      "source": [
        "# 6.2  Regularization and dropout\n",
        "\n",
        "Regularization is a method to reduce the variance of the model to alleviate overfitting. Commonly there are `L1` and `L2` regularization. Besides them, dropout is also commonly used for alleviating overfitting.\n",
        "\n",
        "In this session, we will check both approaches to handle overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "\n",
        "The loss function measures the difference between the output of the model and the real label. Based on this difference, we optimize the model by changing its parameters over time. However, if we leave the parameters free to take any values, they may fit the data too well, making the model overfit. To prevent this, we add a new term to the loss function, called regularization, to impose a constraint on the parameters.\n",
        "Specifically, this constraint forces the parameters to be as small as possible (in other words, as close to zero as possible), thus preventing the parameters from taking any values and overfitting to the data.\n",
        "\n",
        "In this section, we will see how Pytorch implements the L2 regularization, which is usually called `weight decay`.\n",
        "Despite the previously explained concept (that the regulatization is added to the loss), in Pytorch, the weight decay is defined in the optimizer (as we will see below), essentially because, if defined, this term will be added to the loss during the optimization process, regardless of the loss function used.\n"
      ],
      "metadata": {
        "id": "M2kuTGyfr44M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvvC6mg3hrNL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "n_hidden = 200  # number of neurons for the hidden layers\n",
        "max_iter = 2000  # maximum iterations 2000 times\n",
        "disp_interval = 200  # epoch interval for plotting\n",
        "lr_init = 0.01  # learning rate initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a simple synthetic dataset."
      ],
      "metadata": {
        "id": "bd64Tsmvtiwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a batch of virtual data\n",
        "def gen_data(num_data=10, x_range=(-2, 2)):\n",
        "    w = 1.5\n",
        "    train_x = torch.linspace(*x_range, num_data).unsqueeze_(1)\n",
        "    train_y = w*train_x + torch.normal(0, 0.5, size=train_x.size())\n",
        "    test_x = torch.linspace(*x_range, num_data).unsqueeze_(1)\n",
        "    test_y = w*test_x + torch.normal(0, 1.0, size=test_x.size())\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "train_x, train_y, test_x, test_y = gen_data(x_range=(-1, 1))"
      ],
      "metadata": {
        "id": "VPgVtg7Eth_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model."
      ],
      "metadata": {
        "id": "XxBZBOIktm9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCYXBN789mZ_"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, neural_num):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linears = nn.Sequential(\n",
        "            nn.Linear(1, neural_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(neural_num, neural_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(neural_num, neural_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(neural_num, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linears(x)\n",
        "\n",
        "# Instantiate two fully connected networks built above for comparison\n",
        "net_normal = MLP(neural_num=n_hidden)  # this network will be trained without weight decay\n",
        "net_weight_decay = MLP(neural_num=n_hidden)  # this network will be WITH weight decay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a simple Mean Squared Error loss - [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
        "\n",
        "Note that the problem here is regression - not classification!"
      ],
      "metadata": {
        "id": "rW1JPclEvZ_d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmWeppCU6vNC"
      },
      "outputs": [],
      "source": [
        "loss_func = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the SGD optimizers (two in this example). You can check the SGD documentation [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html).\n",
        "\n",
        "The first SGD does not define the `weight_decay` parameter, that is, does not use regularization.\n",
        "On the other hand, the second optimizer explicitly defines the `weight_decay` parameter, setting a value of `1e-2` to it."
      ],
      "metadata": {
        "id": "swFQuxdUtwEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZjnTK829nWY"
      },
      "outputs": [],
      "source": [
        "# net_normal no regularizatio\n",
        "optim_normal = torch.optim.SGD(net_normal.parameters(), lr=lr_init, momentum=0.9)  # parameters of the normal net\n",
        "\n",
        "# net_weight_decay with regularizationï¼Œcoefficient 1e-2\n",
        "optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=lr_init, momentum=0.9, weight_decay=1e-2)  # parameters of the net_weight_decay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have everything we need to train our model."
      ],
      "metadata": {
        "id": "SgQsC8K4vpL4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ8O5Dta-GI_"
      },
      "outputs": [],
      "source": [
        "for epoch in range(max_iter):\n",
        "    # forward\n",
        "    pred_normal, pred_wdecay = net_normal(train_x), net_weight_decay(train_x)  # both networks are being used\n",
        "    # calculating loss based on the output of both networks\n",
        "    loss_normal, loss_wdecay = loss_func(pred_normal, train_y), loss_func(pred_wdecay, train_y)\n",
        "\n",
        "    # backpropagation for both networks\n",
        "    optim_normal.zero_grad()\n",
        "    optim_wdecay.zero_grad()\n",
        "\n",
        "    loss_normal.backward()\n",
        "    loss_wdecay.backward()\n",
        "\n",
        "    optim_normal.step()\n",
        "    optim_wdecay.step()\n",
        "\n",
        "    if (epoch+1) % disp_interval == 0:\n",
        "        print(\"Iteration \" + str(epoch+1) + \"/\" + str(max_iter) + \": loss_normal: \" + str(loss_normal.item()) + \" vs loss_wdecay: \" + str(loss_wdecay.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we got better loss on the training set using the model **without** regularization, huh? Alright! Let's calculate the loss for the test set for comparison."
      ],
      "metadata": {
        "id": "XfXfU1e4wzUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_normal.eval()\n",
        "net_weight_decay.eval()\n",
        "test_pred_normal, test_pred_wdecay = net_normal(test_x), net_weight_decay(test_x)\n",
        "\n",
        "print(loss_func(test_pred_normal, test_y), loss_func(test_pred_wdecay, test_y))"
      ],
      "metadata": {
        "id": "5QtJrBlkwatI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got a better loss on the test set when using the model **with** regularization. That's weird, huh? Essentially, this is because the model trained without regularization overfitted, so it's producing good results for training (since it's very well-fitted on that data), but it can't generalize to new data like the test set, thus producing bad results on that set.\n",
        "\n",
        "Let's plot the train and test data points and the learned curves from both networks for comparison."
      ],
      "metadata": {
        "id": "JAVzdFfRznBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "plt.scatter(train_x.data.numpy(), train_y.data.numpy(), c='blue', s=50, alpha=0.3, label='train')\n",
        "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='red', s=50, alpha=0.3, label='test')\n",
        "plt.plot(test_x.data.numpy(), test_pred_normal.data.numpy(), 'r-', lw=3, label='no weight decay')\n",
        "plt.plot(test_x.data.numpy(), test_pred_wdecay.data.numpy(), 'b--', lw=3, label='weight decay')\n",
        "\n",
        "plt.ylim((-2.5, 2.5))\n",
        "plt.legend(loc='upper left')\n",
        "plt.title(\"Epoch: {}\".format(epoch+1))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "baLzMTaLzlYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3U5sPQ1A7yj"
      },
      "source": [
        "In this plot, we can see that the red solid line (learned by the model **without** regularization) is passing through **all** the blue/training data points - showing how overfitted it is. On the other hand, the red dotted line (learned by the model with regularization), while very similar to the blue line, is slightly more flexible and can generalize better to the test/red data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwtYu3jEA-CA"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "The concept of Dropout is simply to understand: randomly deactivate each neuron in the network.\n",
        "\n",
        "**Random**: There is an inactivation probability (Dropout probability).\n",
        "\n",
        "**Inactivation**: The weight corresponding to the neuron is 0, that is, the neuron does not contribute to the network's outcome.\n",
        "\n",
        "By preventing the network from relying too much on specific patterns/neurons, it generalizes better to unseen data, alleviating fitting problem.\n",
        "\n",
        "The following code is similar to the above to compare Dropout with non-Dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kdH2AsOCN6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "n_hidden = 200\n",
        "max_iter = 2000\n",
        "disp_interval = 400\n",
        "lr_init = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_data(num_data=20, x_range=(-1, 1)):\n",
        "    w = 1.5\n",
        "    train_x = torch.linspace(*x_range, num_data).unsqueeze_(1)\n",
        "    train_y = w*train_x + torch.normal(0, 0.5, size=train_x.size())\n",
        "    test_x = torch.linspace(*x_range, num_data).unsqueeze_(1)\n",
        "    test_y = w*test_x + torch.normal(0, 1.0, size=test_x.size())\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "train_x, train_y, test_x, test_y = gen_data(x_range=(-1, 1))"
      ],
      "metadata": {
        "id": "zoGMMbKS3FaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The network architecture is defined below. Note the [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layers.\n",
        "These layers receive a parameter as input that is \"probability of an element to be zeroed\".\n",
        "\n",
        "In the code below, two networks are instantiated - one with dropout probability 0.5 and the other with probability 0.0 (meaning that, dropouts will not do anything in this case)."
      ],
      "metadata": {
        "id": "XyDJ7rlk3nqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, neural_num, d_prob=0.5):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linears = nn.Sequential(\n",
        "\n",
        "            nn.Linear(1, neural_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(d_prob),  # dropout\n",
        "            nn.Linear(neural_num, neural_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(d_prob),  # dropout\n",
        "            nn.Linear(neural_num, neural_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(d_prob),  # dropout\n",
        "            nn.Linear(neural_num, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linears(x)\n",
        "\n",
        "net_prob_0 = MLP(neural_num=n_hidden, d_prob=0.)\n",
        "net_prob_05 = MLP(neural_num=n_hidden, d_prob=0.5)"
      ],
      "metadata": {
        "id": "Am9TO2_H3HLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim_normal = torch.optim.SGD(net_prob_0.parameters(), lr=lr_init, momentum=0.9)\n",
        "optim_reglar = torch.optim.SGD(net_prob_05.parameters(), lr=lr_init, momentum=0.9)\n",
        "\n",
        "loss_func = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "rTQCvc3o3Igd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(max_iter):\n",
        "\n",
        "    pred_normal, pred_dropout = net_prob_0(train_x), net_prob_05(train_x)\n",
        "    loss_normal, loss_dropout = loss_func(pred_normal, train_y), loss_func(pred_dropout, train_y)\n",
        "\n",
        "    optim_normal.zero_grad()\n",
        "    optim_reglar.zero_grad()\n",
        "\n",
        "    loss_normal.backward()\n",
        "    loss_dropout.backward()\n",
        "\n",
        "    optim_normal.step()\n",
        "    optim_reglar.step()\n",
        "\n",
        "    if (epoch+1) % disp_interval == 0:\n",
        "        print(\"Iteration \" + str(epoch+1) + \"/\" + str(max_iter) + \": loss_normal: \" + str(loss_normal.item()) + \" vs loss_dropout: \" + str(loss_wdecay.item()))"
      ],
      "metadata": {
        "id": "MnW0C6Bw3KMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, better loss on the training set with the model **without** dropout!"
      ],
      "metadata": {
        "id": "Bi_TAvNq5fuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_prob_0.eval()\n",
        "net_prob_05.eval()\n",
        "\n",
        "test_pred_prob_0, test_pred_prob_05 = net_prob_0(test_x), net_prob_05(test_x)\n",
        "\n",
        "print(loss_func(test_pred_prob_0, test_y), loss_func(test_pred_prob_05, test_y))"
      ],
      "metadata": {
        "id": "F_GXitMP3LEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But **again**, better loss on the test set using the model trained **with** dropout!!!"
      ],
      "metadata": {
        "id": "KfYBlMzq5uIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "plt.clf()\n",
        "plt.scatter(train_x.data.numpy(), train_y.data.numpy(), c='blue', s=50, alpha=0.3, label='train')\n",
        "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='red', s=50, alpha=0.3, label='test')\n",
        "plt.plot(test_x.data.numpy(), test_pred_prob_0.data.numpy(), 'r-', lw=3, label='d_prob_0')\n",
        "plt.plot(test_x.data.numpy(), test_pred_prob_05.data.numpy(), 'b--', lw=3, label='d_prob_05')\n",
        "\n",
        "plt.ylim((-2.5, 2.5))\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "6hkIKwca3WqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How overfitted is the red line (learned by the model without dropout)?"
      ],
      "metadata": {
        "id": "IaS8eHDR5_vh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}