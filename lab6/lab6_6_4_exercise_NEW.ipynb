{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMFrkmpGbUDO"
      },
      "source": [
        "\n",
        "# 6.4 Exercises\n",
        "\n",
        "To get better classification results on RNN, you can explore the following RNN architectures:\n",
        "\n",
        "   - 1) Add more ``nn.RNN`` layers\n",
        "   - 2) Add the ``nn.LSTM`` layers\n",
        "   - 3) Add the ``nn.GRU`` layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iICLoyShbUDS"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# import library\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "# import ooperating system module\n",
        "import os\n",
        "# glob module finds all the pathnames matching a specified pattern\n",
        "import glob\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# Find letter index from all_letters, index of n_letters\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "def label_from_output(output, output_labels):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    label_i = top_i[0].item()\n",
        "    return output_labels[label_i], label_i\n",
        "\n",
        "class NamesDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir  # for provenance of the dataset\n",
        "        labels_set = set()  # set of all classes\n",
        "\n",
        "        self.data = []\n",
        "        self.data_tensors = []\n",
        "        self.labels = []\n",
        "        self.labels_tensors = []\n",
        "\n",
        "        # read all the ``.txt`` files in the specified directory\n",
        "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
        "        for filename in text_files:\n",
        "            label = os.path.splitext(os.path.basename(filename))[0]\n",
        "            labels_set.add(label)\n",
        "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "            for name in lines:\n",
        "                self.data.append(name)\n",
        "                self.data_tensors.append(lineToTensor(name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        # This last bit converts the labels from string to number\n",
        "        # Using the idea of encoding we saw during the lectures\n",
        "        self.labels_uniq = list(labels_set)\n",
        "        for idx in range(len(self.labels)):\n",
        "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
        "            self.labels_tensors.append(temp_tensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_item = self.data[idx]\n",
        "        data_label = self.labels[idx]\n",
        "        data_tensor = self.data_tensors[idx]\n",
        "        label_tensor = self.labels_tensors[idx]\n",
        "\n",
        "        return label_tensor, data_tensor, data_label, data_item\n",
        "\n",
        "alldata = NamesDataset(\"data/names\")\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
        "\n",
        "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
      ],
      "metadata": {
        "id": "7s48m4tR9Wo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GALTmJFPbUDV"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Create and train an architecture with more ``nn.RNN`` layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n13l0IbSbUDZ"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    ## CODE HERE\n",
        "\n",
        "n_letters =57\n",
        "n_categories =18\n",
        "print(\"number of letters: \", n_letters)\n",
        "print(\"number of categories: \", n_categories)\n",
        "\n",
        "# set the hidden neurons\n",
        "n_hidden = 256\n",
        "# number of RNN layers\n",
        "num_layers = 2\n",
        "# instance a RNN model\n",
        "rnn = RNN(n_letters, n_hidden, num_layers, n_categories)\n",
        "\n",
        "print(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# If you set this too high, it might explode. If too low, it might not learn\n",
        "learning_rate = 0.15\n",
        "\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Xb-TuLui8o77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(rnn, training_data, n_epoch=10, n_batch_size = 64):\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "    rnn.train()\n",
        "\n",
        "    for iter in range(1, n_epoch + 1):\n",
        "        rnn.zero_grad() # clear the gradients\n",
        "\n",
        "        # create some minibatches\n",
        "        # we cannot use dataloaders because each of our names is a different length!!\n",
        "        batches = list(range(len(training_data)))\n",
        "        random.shuffle(batches)\n",
        "        batches = np.array_split(batches, len(batches) // n_batch_size )\n",
        "\n",
        "        for idx, batch in enumerate(batches):\n",
        "            batch_loss = 0\n",
        "            for i in batch: # for each example in this batch\n",
        "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
        "                output = rnn.forward(text_tensor)\n",
        "                loss = criterion(output, label_tensor)\n",
        "                batch_loss += loss\n",
        "\n",
        "            # optimize parameters\n",
        "            batch_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)  # this is to facilitate convergence\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            current_loss += batch_loss.item() / len(batch)\n",
        "\n",
        "        all_losses.append(current_loss / len(batches))\n",
        "        if iter % 5 == 0:\n",
        "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
        "        current_loss = 0\n",
        "\n",
        "    return all_losses\n",
        "\n",
        "def evaluate(rnn, testing_data, classes):\n",
        "    confusion = torch.zeros(len(classes), len(classes))\n",
        "\n",
        "    rnn.eval() # set to eval mode\n",
        "    with torch.no_grad(): # do not record the gradients during eval phase\n",
        "        for i in range(len(testing_data)):\n",
        "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
        "            output = rnn(text_tensor)\n",
        "            guess, guess_i = label_from_output(output, classes)\n",
        "            label_i = classes.index(label)\n",
        "            confusion[label_i][guess_i] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(len(classes)):\n",
        "        denom = confusion[i].sum()\n",
        "        if denom > 0:\n",
        "            confusion[i] = confusion[i] / denom\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(confusion.cpu().numpy()) # numpy uses cpu here so we need to use a cpu version\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
        "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
        "\n",
        "    # Force label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sQ-Wwm4L-Jl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_losses = train(rnn, train_set, n_epoch=27, n_batch_size=64)"
      ],
      "metadata": {
        "id": "jVTJ5ag7-O5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
      ],
      "metadata": {
        "id": "MIuuXBao-Q5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xG_y53_bUDb"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Create and train an architecture using [LSTM layers](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji7icgnKbUDc"
      },
      "outputs": [],
      "source": [
        "# build Recurrent neural network using nn.LSTM() (one to one)\n",
        "class LSTMRNN(nn.Module):\n",
        "    ## CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYSO2rDIbUDg"
      },
      "outputs": [],
      "source": [
        "n_letters =57\n",
        "n_categories =18\n",
        "print(\"number of letters: \", n_letters)\n",
        "print(\"number of categories: \", n_categories)\n",
        "\n",
        "# set the hidden neurons\n",
        "n_hidden = 256\n",
        "# number of RNN layers\n",
        "num_layers = 2\n",
        "\n",
        "# instance a LSTM model\n",
        "rnn = LSTMRNN(n_letters, n_hidden, num_layers, n_categories)\n",
        "\n",
        "print(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# If you set this too high, it might explode. If too low, it might not learn\n",
        "learning_rate = 0.15\n",
        "\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "S6AFYB64DzXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(rnn, training_data, n_epoch=10, n_batch_size = 64):\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "    rnn.train()\n",
        "\n",
        "    for iter in range(1, n_epoch + 1):\n",
        "        rnn.zero_grad() # clear the gradients\n",
        "\n",
        "        # create some minibatches\n",
        "        # we cannot use dataloaders because each of our names is a different length!!\n",
        "        batches = list(range(len(training_data)))\n",
        "        random.shuffle(batches)\n",
        "        batches = np.array_split(batches, len(batches) // n_batch_size )\n",
        "\n",
        "        for idx, batch in enumerate(batches):\n",
        "            batch_loss = 0\n",
        "            for i in batch: # for each example in this batch\n",
        "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
        "                output = rnn.forward(text_tensor)\n",
        "                loss = criterion(output, label_tensor)\n",
        "                batch_loss += loss\n",
        "\n",
        "            # optimize parameters\n",
        "            batch_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)  # this is to facilitate convergence\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            current_loss += batch_loss.item() / len(batch)\n",
        "\n",
        "        all_losses.append(current_loss / len(batches))\n",
        "        if iter % 5 == 0:\n",
        "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
        "        current_loss = 0\n",
        "\n",
        "    return all_losses\n",
        "\n",
        "def evaluate(rnn, testing_data, classes):\n",
        "    confusion = torch.zeros(len(classes), len(classes))\n",
        "\n",
        "    rnn.eval() # set to eval mode\n",
        "    with torch.no_grad(): # do not record the gradients during eval phase\n",
        "        for i in range(len(testing_data)):\n",
        "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
        "            output = rnn(text_tensor)\n",
        "            guess, guess_i = label_from_output(output, classes)\n",
        "            label_i = classes.index(label)\n",
        "            confusion[label_i][guess_i] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(len(classes)):\n",
        "        denom = confusion[i].sum()\n",
        "        if denom > 0:\n",
        "            confusion[i] = confusion[i] / denom\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(confusion.cpu().numpy()) # numpy uses cpu here so we need to use a cpu version\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
        "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
        "\n",
        "    # Force label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "giUXe7-MDzXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_losses = train(rnn, train_set, n_epoch=27, n_batch_size=64)"
      ],
      "metadata": {
        "id": "6zLEpQA1DzXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
      ],
      "metadata": {
        "id": "MY-ebNL6DzXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP2YJeFqbUDj"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Create and train an architecture using [GRU layers](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NCWtRvobUDk"
      },
      "outputs": [],
      "source": [
        "# Recurrent neural network, nn.GRU() (one to one)\n",
        "class GRURNN(nn.Module):\n",
        "    ## CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TuxgV__bUDm"
      },
      "outputs": [],
      "source": [
        "n_letters =57\n",
        "n_categories =18\n",
        "print(\"number of letters: \", n_letters)\n",
        "print(\"number of categories: \", n_categories)\n",
        "\n",
        "# set the hidden neurons\n",
        "n_hidden = 256\n",
        "# number of RNN layers\n",
        "num_layers = 2\n",
        "\n",
        "# instance a RNN model\n",
        "rnn = GRURNN(n_letters, n_hidden, num_layers, n_categories)\n",
        "print(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# If you set this too high, it might explode. If too low, it might not learn\n",
        "learning_rate = 0.15\n",
        "\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "3UeotMpYD0r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(rnn, training_data, n_epoch=10, n_batch_size = 64):\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "    rnn.train()\n",
        "\n",
        "    for iter in range(1, n_epoch + 1):\n",
        "        rnn.zero_grad() # clear the gradients\n",
        "\n",
        "        # create some minibatches\n",
        "        # we cannot use dataloaders because each of our names is a different length!!\n",
        "        batches = list(range(len(training_data)))\n",
        "        random.shuffle(batches)\n",
        "        batches = np.array_split(batches, len(batches) // n_batch_size )\n",
        "\n",
        "        for idx, batch in enumerate(batches):\n",
        "            batch_loss = 0\n",
        "            for i in batch: # for each example in this batch\n",
        "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
        "                output = rnn.forward(text_tensor)\n",
        "                loss = criterion(output, label_tensor)\n",
        "                batch_loss += loss\n",
        "\n",
        "            # optimize parameters\n",
        "            batch_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)  # this is to facilitate convergence\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            current_loss += batch_loss.item() / len(batch)\n",
        "\n",
        "        all_losses.append(current_loss / len(batches))\n",
        "        if iter % 5 == 0:\n",
        "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
        "        current_loss = 0\n",
        "\n",
        "    return all_losses\n",
        "\n",
        "def evaluate(rnn, testing_data, classes):\n",
        "    confusion = torch.zeros(len(classes), len(classes))\n",
        "\n",
        "    rnn.eval() # set to eval mode\n",
        "    with torch.no_grad(): # do not record the gradients during eval phase\n",
        "        for i in range(len(testing_data)):\n",
        "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
        "            output = rnn(text_tensor)\n",
        "            guess, guess_i = label_from_output(output, classes)\n",
        "            label_i = classes.index(label)\n",
        "            confusion[label_i][guess_i] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(len(classes)):\n",
        "        denom = confusion[i].sum()\n",
        "        if denom > 0:\n",
        "            confusion[i] = confusion[i] / denom\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(confusion.cpu().numpy()) # numpy uses cpu here so we need to use a cpu version\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
        "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
        "\n",
        "    # Force label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UUI0efipD0r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_losses = train(rnn, train_set, n_epoch=27, n_batch_size=64)"
      ],
      "metadata": {
        "id": "gA-OKaM2D0sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
      ],
      "metadata": {
        "id": "-a7dfF9rD0sD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GALTmJFPbUDV"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}