{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1gNgbkdlxon"
      },
      "source": [
        "# Build the Neural Network\n",
        "To build the neural network, we need the framework of [PyTorch](https://pytorch.org/). You can refer to the website for installation and other informations.\n",
        "After installation PyTorch (if you are using Google Colab, you do NOT need to worry about installation), you can call torch to build your own models. Now, let's start.\n",
        "\n",
        "In one of the lectures, we saw that a simple feedforward neural network includes the:\n",
        "1. input layer,\n",
        "2. hidden layer, and\n",
        "3. output layer.\n",
        "\n",
        "In Pytorch, the [torch.nn](https://pytorch.org/docs/stable/nn.html) can be used to constructed the neural network models.\n",
        "The [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is the base class for all neural network modules, which contains layers, and a method ``forward(input)`` that returns the <b>output</b>.\n",
        "Let's have a look the details.\n",
        "\n",
        "In the next section, we'll build a neural network to classify images in the FashionMNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_1UNIuslxoo"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Load the package\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "# torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDmBMdezlxop"
      },
      "source": [
        "## Get Device for Training model\n",
        "\n",
        "You can train the model on a hardware accelerator like the GPU, if it is available.\n",
        "Let's check to see if\n",
        "[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we\n",
        "continue to use the CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htn4hdl9lxop"
      },
      "outputs": [],
      "source": [
        "# choose the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the output above shows 'cpu', it means that PyTorch was unable to detect GPUs. If you are running this using Google Colab, you can change it to use GPU for free. Just go to Runtime > Change Runtime Type and choose GPU.\n",
        "Note that there is a limit to GPU usage and once you reach that limit, you will need to pay. However, we will not go into that limit in this section."
      ],
      "metadata": {
        "id": "Q8doBq8t4vke"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpxZUavAlxop"
      },
      "source": [
        "## 1- Define the Neural Network\n",
        "\n",
        "We define our neural network by subclassing ``nn.Module``, and\n",
        "initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n",
        "the operations on input data in the ``forward`` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4uj1pChlxoq"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        # here we define the structure\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),  # 10 here means that there are 10 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # and here we define the feed-forward step\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls24cvOilxoq"
      },
      "source": [
        "Now, we create an instance of ``NeuralNetwork``, i.e. ``model``, and move it to the ``device`` (meaning 'cpu' or 'gpu'), then print its structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8TRUuNAlxor"
      },
      "outputs": [],
      "source": [
        "# create \"model\" as network structure\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhjst8CDzBMn"
      },
      "source": [
        "\n",
        "To use the model, we feed it a random input. This executes the model's ``forward``,\n",
        "along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
        "Do not call ``model.forward()`` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor:\n",
        "- the first dimension corresponds to each output of 10 raw predicted values for each class, and\n",
        "- the second dimension corresponds to the individual values of each output.\n",
        "\n",
        "We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Astd61GfzBMr"
      },
      "outputs": [],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)  # this generates the predictions\n",
        "print(logits)\n",
        "print(logits.size())  # 1 sample x 10 classes\n",
        "\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAQCNK5izBMt"
      },
      "source": [
        "#### Gradient\n",
        "\n",
        "Let's now zero the gradient buffers of all parameters and then perform a ackpropagation with random gradients.\n",
        "Although this may not make a lot of sense now, it will be useful later when we train the model for real.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W3MI6TWzBMu"
      },
      "outputs": [],
      "source": [
        "# model gradient initialize\n",
        "model.zero_grad()\n",
        "logits.backward(torch.randn(1,10))\n",
        "print(logits.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBUzI-9hzBMv"
      },
      "source": [
        "#### Loss Function\n",
        "\n",
        "A loss function takes the `(output, target)` pair of inputs, and computes a\n",
        "value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different\n",
        "[loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) in the nn package.\n",
        "A simple loss is: ``nn.MSELoss``, which computes the mean-squared error\n",
        "between the output and the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVahVj_kzBMw"
      },
      "outputs": [],
      "source": [
        "# a dummy target, for example\n",
        "target = torch.randn(10)\n",
        "\n",
        "# make it the same shape as output\n",
        "target = target.view(1, -1)\n",
        "print(target.size())\n",
        "\n",
        "# define the loss function\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(logits, target)\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGrEGhEolxos"
      },
      "source": [
        "## 2- Each Layer Analysis\n",
        "\n",
        "Now, we analyze each layer in the model.\n",
        "To illustrate it, we will take a sample minibatch of 3 with size 28x28 using [nn.Rand](https://pytorch.org/docs/stable/generated/torch.rand.html). Then we pass it through the network and do the further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81WSWX8rlxos"
      },
      "outputs": [],
      "source": [
        "input = torch.rand(3,28,28)\n",
        "print(input.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAjZq5Knlxos"
      },
      "source": [
        "### nn.Flatten\n",
        "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 28x28 input into a contiguous array of 784 values (\n",
        "the minibatch dimension (at dim=0 / first dimension) is maintained).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMPR7R1wlxos"
      },
      "outputs": [],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_input = flatten(input)\n",
        "print(flat_input.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KYOKLlklxos"
      },
      "source": [
        "### nn.Linear\n",
        "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "is a module that applies a linear transformation on the input using its stored weights and biases.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHdpLSSplxot"
      },
      "outputs": [],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)  # this will convert 28*28 values into 20\n",
        "hidden1 = layer1(flat_input)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPxymVzxlxot"
      },
      "source": [
        "### nn.ReLU\n",
        "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
        "They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n",
        "learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our\n",
        "linear layers, but there's [other activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) to introduce non-linearity in your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hHIrZkolxot"
      },
      "outputs": [],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgzfQr1ilxot"
      },
      "source": [
        "### nn.Sequential\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered\n",
        "container of modules. The data is passed through all the modules in the same order as defined. You can use\n",
        "sequential containers to put together a quick network like ``seq_modules``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-niVeeilxot"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input = torch.rand(3,28,28)\n",
        "\n",
        "logits = seq_modules(input)\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tn0oRqolxot"
      },
      "source": [
        "### nn.Softmax\n",
        "The last linear layer of the neural network returns `logits` - raw values in $[-\\infty, \\infty]$ - which are passed to the\n",
        "[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax) module. The logits are scaled to values\n",
        "[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n",
        "which the values must sum to 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xewj11rGlxot"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "print(pred_probab)\n",
        "print(pred_probab.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1aIztATzBM7"
      },
      "source": [
        "## 3- Model Parameters\n",
        "\n",
        "In the model, we define the ``forward function`` in ``NeuralNetwork``, the backward function can be automatically calculated by ``autograd`` in PyTorch. No need to worry about the `autograd`, just think that it will work all the backpropagation for us.\n",
        "\n",
        "Many layers of a model are *parameterized*, i.e. have associated weights\n",
        "and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n",
        "tracks all fields defined inside your model object, and makes all parameters\n",
        "accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n",
        "\n",
        "In this example, we iterate over each learnable parameter, and print its size and its values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEBdtI5LzBM8"
      },
      "outputs": [],
      "source": [
        "params = list(model.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())\n",
        "\n",
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKJHjPhdzBM9"
      },
      "source": [
        "## 4- Update the Weights of the networks\n",
        "After we obtain the parameters by feeding input into network, we can update the parameters based on the optimizer.\n",
        "\n",
        "In practice, the simplest update rule is the Stochastic Gradient Descent (SGD):\n",
        "\n",
        "$$ weight = weight - learning rate * gradient $$\n",
        "\n",
        "By updating the parameters, we complete the whole training process and achieve the best parameters.\n",
        "\n",
        "Now, let's have a look a simple example with [torch.optim](https://pytorch.org/docs/stable/optim.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVOWPSdzBM-"
      },
      "outputs": [],
      "source": [
        "#  import the package\n",
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer with SGD and learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# a dummy target, for example\n",
        "target = torch.randn(10)\n",
        "# make it the same shape as output\n",
        "target = target.view(1, -1)   # 1x10\n",
        "print(target.size())\n",
        "\n",
        "# in the training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = model(input)  # prediction\n",
        "loss = criterion(output, target)  # calculate the loss\n",
        "loss.backward()  # compute the backpropagation\n",
        "optimizer.step()    # Does the update\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK7Uw7Dalxou"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KPAfqzqlxou"
      },
      "source": [
        "## Further Reading\n",
        "You can refer the following website for further information.\n",
        "\n",
        "- [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n",
        "- [tutorials](https://pytorch.org/tutorials/)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c37791b62b9d5c239fb0fc5acdaf3bd7e92ad96e6d57cd91101e1e69e5d3005"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}